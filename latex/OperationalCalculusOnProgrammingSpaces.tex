\input{../latexCommon/preamble.tex}
\title{Operational Calculus on Programming Spaces}
\author{\v{Z}iga Sajovic, Martin Vuk}

\begin{document}

\maketitle

\section{Introduction}

Von Neumann languages do not have useful properties for reasoning about programs. Axiomatic and denotational semantics are precise tools for describing and understanding conventional programs, but they only talk about them and cannot alter their ungainly
properties \cite{backus}. This issue raised by John Backus has partially been addressed by algebraic data types employed by functional programming, where a mapping has been shown between grammars and semirings; for an example of how this algebraic view is used to show an isomorphism between data types, see \cite{7Trees}. Yet due to the lack of inverses (hence the semiring structure) we remain limited in the algebraic manipulations we are allowed to employ; for an explanation via a connection between objects of categories and complex numbers see \cite{complexCat}. This leaves one intrigued, but wanting, as the existence of inverses is what gives algebra its true power -- the ability to solve equations.

In recent times, names like \emph{Differentiable Programming} and \emph{Software $2.0$} have attached themselves to Deep learning, as it has shown itself to be more than a collection of machine learning algorithms; it is emerging as a new programming paradigm.
But because the field is still in its youth, most of the advances come as a result of empirical investigations.
Yet, as it is founded on rigorous mathematical objects, it offers an opportunity to be formalized as a language.
And as it is rooted in tensor algebra, it holds the ability to address the outlined issues, because \emph{unlike von Neumann languages, the language of ordinary algebra is suitable both for stating its laws and for transforming
an equation into its solution, all within the language} \cite{backus}.
Furthermore, due of its innate relationship with calculus, a language encompassing it would serve as a formal gateway of analysis into programming.

TODO: what the paper contains etc.

\section{Computer Programs as Maps on a Vector Space}

We will model computer programs as maps on a vector space. If
we only focus on the real valued variables (of type \texttt{float} or
\texttt{double}), the state of the memory can be seen as a high
dimensional vector\footnote{We assume the variables of interest to be of type \texttt{float} for
  simplicity. Theoretically any field can be used instead of $\RR$.}. 
A set of all the possible states of the program's memory,
can be modeled by a finite dimensional real vector space $\VV\equiv \RR^n$. We
will call $\VV$ the \emph{memory space of the program}. The effect of a computer
program on its memory space $\VV$, can be described by a map
\begin{equation}
  \label{eq:map}
  P:\VV\to \VV.
\end{equation}
A programming space is a space of maps $\VV\to\VV$ that can be implemented as a
program in specific programming language. 
\begin{definition}[Euclidean machine] The tuple $(\VV,\F)$ is an Euclidean machine, where
  \begin{itemize}
  \item
  $\VV$ is a finite dimensional vector space over a complete field $K$, serving
  as memory\footnote{In most applications the field $K$ will
    be $\RR$}
  \item
  $\F< \VV^\VV$ is a subspace of the space of maps $\VV\to \VV$, called \emph{programming space}, serving as actions on the memory.
  \end{itemize}  
\end{definition}

At first glance, the \emph{Euclidean machine} seems like a description of functional programming, with its compositions inherited from $\F$. An intended impression, as we wish for the \emph{Euclidean machine} to build on its elegance. But note that in the coming section an additional restriction is imposed on $\F$; that of its elements being differentiable.

\section{Differentiable Maps and Programs}
To define differentiable programs, let us first recall some
definitions from multivariate calculus.
\begin{definition}[Derivative]
  Let $V,U$ be Banach spaces. A map $P:V\to U$ is differentiable at a point
  $\x\in V$, if there exists a linear bounded operator $TP_\x:V\to U$ such that
  \begin{equation}
    \label{eq:frechet}
    \lim_{\h\to 0}\frac{\|P(\x+\h)-P(\x)-TP_\x(\h)\|}{\|\h\|} = 0.
  \end{equation}
  The map $TP_\x$ is called the \emph{Fréchet derivative} of the map $P$ at the
  point $\x$.
\end{definition}
For maps $\RR^n\to \RR^m$ Fréchet derivative can be expressed by multiplication
of vector $\h$ by the Jacobi matrix $\mathbf{J}_{P,\x}$ of partial
derivatives of the components of the map $P$
\begin{equation*}
  T_\x P(\h) = \mathbf{J}_{P,\x}\cdot \h.
\end{equation*}

We assume for the remainder of this section that the map $P:V\to U$ is
differentiable for all $\x\in V$. The derivative defines a map from $V$ to
linear bounded maps from $V$ to $U$. We further assume $U$ and $V$ are finite
dimensional. Then the space of linear maps from $V$ to $U$ is isomorphic to
tensor product $U\otimes V^*$, where the isomorphism is given by the
tensor contraction, sending a simple tensor $\uu\otimes f\in U\otimes
V^*$ to a linear map
 \begin{equation}
   \label{eq:lin_tenzor}
   \uu\otimes f:\x \mapsto f(\x)\cdot \uu.
 \end{equation}
The derivative defines a map
\begin{eqnarray}
  \label{eq:odvod_preslikava}
  \D P&:& V\to U\otimes V^*\\
  \D P&:& \x \mapsto T_\x P.
\end{eqnarray}
One can consider the differentiability of the derivative itself $\D P$ by
looking at it as a map \eqref{eq:odvod_preslikava}. This leads to the definition
of the higher derivatives.
\begin{definition}[higher derivatives]
  \label{def:higher_derivatives}
  Let $P:V\to U$ be a map from vector space $V$ to vector space $U$. 
The derivative $\D^k P$ of order $k$ of the map $P$ is the map
\begin{eqnarray}\label{eq:partial}
    \label{eq:visji_odvod}
    \D^kP&:&V\to U\otimes (V^*)^{\otimes k}\\
    \D^kP&:&\x\mapsto T_\x\left( \D^{k-1}P \right)
  \end{eqnarray}
\end{definition} 
\begin{remark}
  For the sake of clarity, we assumed in the definition above, that the map $P$
  as well as all its derivatives are differentiable at all points $\x$. If this
  is not the case, 
  definitions above can be done locally, which would introduce mostly technical
  difficulties. 
\end{remark}
Let $\e_1,\ldots,\e_n$ be a basis of $U$ and $x_1,\ldots x_m$ the basis of
$V^*$. Denote by $P_i=x_i\circ P$ the $i-th$ component of the map
$P$ according to the basis $\{\e_i\}$ of $U$.
Then $\D^kP$  can be defined in terms of
directional (partial) derivatives by the formula
\begin{equation}\label{eq:d}
  \partial^kP=\sum_{\forall_{i,\alpha}}\frac{\partial^k P_i}{\partial
      x_{\alpha_1}\ldots \partial x_{\alpha_k}}\e_i\otimes
    dx_{\alpha_1}\otimes\ldots \otimes dx_{\alpha_k}.
\end{equation}

\printbibliography

\end{document}